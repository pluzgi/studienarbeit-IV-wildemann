{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, RobustScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.model_selection import learning_curve, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Laden der Daten\n",
    "TEMP_PATH = os.path.join(\"..\", \"temp\")\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "train_df = pd.read_pickle(os.path.join(TEMP_PATH, \"train.pickle\"))\n",
    "test_df = pd.read_pickle(os.path.join(TEMP_PATH, \"test.pickle\"))\n",
    "\n",
    "# Überprüfen der Daten auf Überlappungen oder Lecks\n",
    "common_rows = pd.merge(train_df, test_df)\n",
    "if not common_rows.empty:\n",
    "    print(f\"Es gibt {len(common_rows)} identische Zeilen in den Trainings- und Testdaten.\")\n",
    "else:\n",
    "    print(\"Keine identischen Zeilen gefunden.\")\n",
    "\n",
    "# Überprüfen der NaN-Werte\n",
    "def print_nan_counts(df, message=\"\"):\n",
    "    print(message)\n",
    "    nan_counts = df.isna().sum()\n",
    "    if nan_counts.any():\n",
    "        print(nan_counts[nan_counts > 0])\n",
    "    else:\n",
    "        print(\"No NaNs present.\")\n",
    "\n",
    "# Entfernen des %-Zeichens und Konvertierung zu FLOAT\n",
    "def convert_percent_columns(df, columns):\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col].str.rstrip('%'), errors='coerce')\n",
    "    return df\n",
    "\n",
    "percentage_columns = ['% Engaged sessions (GA4)', 'Interaction rate']\n",
    "train_df = convert_percent_columns(train_df, percentage_columns)\n",
    "test_df = convert_percent_columns(test_df, percentage_columns)\n",
    "\n",
    "# Check for NaNs after converting percentage columns\n",
    "print_nan_counts(train_df, \"NaN counts in train_df after converting percentage columns:\")\n",
    "print_nan_counts(test_df, \"NaN counts in test_df after converting percentage columns:\")\n",
    "\n",
    "\n",
    "# Anpassung der Auswahl der numerischen und kategorialen Variablen\n",
    "numerical_features = [\n",
    "    'Hour_Clicks', 'Hour_Cost', 'Hour_Impr.', 'Keyword_Impr', 'Keyword_Clicks', \n",
    "    'Keyword_Cost', 'Asset_Impr', 'Asset_word_count', 'Search keyword_word_count'\n",
    "]\n",
    "\n",
    "categorical_features = ['Ad_group_x', 'Asset type', 'Day_of_week', 'Hour_of_day', 'Search keyword', 'Asset']\n",
    "\n",
    "# Konvertierung zu numerischen und string Werten\n",
    "for feature in numerical_features:\n",
    "    train_df[feature] = pd.to_numeric(train_df[feature], errors='coerce')\n",
    "    test_df[feature] = pd.to_numeric(test_df[feature], errors='coerce')\n",
    "\n",
    "for feature in categorical_features:\n",
    "    train_df[feature] = train_df[feature].astype(str)\n",
    "    test_df[feature] = test_df[feature].astype(str)\n",
    "\n",
    "# Preprocessing Pipeline für numerische und kategoriale Features\n",
    "num_transformers = [\n",
    "    ('hour_clicks_scaler', StandardScaler(), ['Hour_Clicks']),\n",
    "    ('hour_cost_scaler', StandardScaler(), ['Hour_Cost']),\n",
    "    ('hour_impr_scaler', MinMaxScaler(), ['Hour_Impr.']),\n",
    "    ('keyword_impr_scaler', MinMaxScaler(), ['Keyword_Impr']),\n",
    "    ('keyword_clicks_scaler', StandardScaler(), ['Keyword_Clicks']),\n",
    "    ('keyword_cost_scaler', StandardScaler(), ['Keyword_Cost']),\n",
    "    ('asset_impr_scaler', StandardScaler(), ['Asset_Impr']),\n",
    "    ('asset_word_count_scaler', RobustScaler(), ['Asset_word_count']),\n",
    "    ('search_keyword_word_count_scaler', RobustScaler(), ['Search keyword_word_count'])\n",
    "]\n",
    "\n",
    "cat_transformers = [\n",
    "    (f\"{feature}_ohe\", OneHotEncoder(handle_unknown='ignore', sparse_output=False), [feature]) for feature in categorical_features\n",
    "]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=num_transformers + cat_transformers,\n",
    "    remainder='drop',\n",
    "    sparse_threshold=0\n",
    ")\n",
    "\n",
    "# Definiere die Pipelines für Random Forest und Gradient Boosting\n",
    "rf_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"regressor\", RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Training Random Forest für CTR\n",
    "X_train_ctr, X_test_ctr, y_train_ctr, y_test_ctr = train_test_split(\n",
    "    train_df.drop(columns=['CTR', 'Avg. CPC']), \n",
    "    train_df['CTR'], \n",
    "    test_size=0.2, random_state=42\n",
    ")\n",
    "rf_pipeline.fit(X_train_ctr, y_train_ctr)\n",
    "rf_train_score_ctr = rf_pipeline.score(X_train_ctr, y_train_ctr)\n",
    "rf_test_score_ctr = rf_pipeline.score(X_test_ctr, y_test_ctr)\n",
    "print(f\"Random Forest - CTR Train Score: {rf_train_score_ctr}, Test Score: {rf_test_score_ctr}\")\n",
    "\n",
    "# Training Random Forest für Avg. CPC\n",
    "X_train_cpc, X_test_cpc, y_train_cpc, y_test_cpc = train_test_split(\n",
    "    train_df.drop(columns=['CTR', 'Avg. CPC']), \n",
    "    train_df['Avg. CPC'], \n",
    "    test_size=0.2, random_state=42\n",
    ")\n",
    "rf_pipeline.fit(X_train_cpc, y_train_cpc)\n",
    "rf_train_score_cpc = rf_pipeline.score(X_train_cpc, y_train_cpc)\n",
    "rf_test_score_cpc = rf_pipeline.score(X_test_cpc, y_test_cpc)\n",
    "print(f\"Random Forest - Avg. CPC Train Score: {rf_train_score_cpc}, Test Score: {rf_test_score_cpc}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-Parameter Tuning Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definieren des Parameter-Grids\n",
    "param_grid = {\n",
    "    'regressor__n_estimators': [100, 200],\n",
    "    'regressor__max_depth': [None, 10, 20],\n",
    "    'regressor__min_samples_split': [2, 5, 10],\n",
    "    'regressor__max_features': ['auto', 'sqrt']\n",
    "}\n",
    "\n",
    "# Erstellen einer neuen Pipeline für GridSearchCV\n",
    "grid_search = GridSearchCV(rf_pipeline, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train_cpc, y_train_cpc)\n",
    "\n",
    "# Beste Parameter und Score ausgeben\n",
    "print(\"Beste Hyperparameter:\", grid_search.best_params_)\n",
    "print(\"Bester Modell Score (neg_mean_squared_error):\", grid_search.best_score_)\n",
    "\n",
    "# Verwenden des besten Modells\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "best_rf_model.fit(X_train_cpc, y_train_cpc)\n",
    "print(\"Verbesserte Test Score für Avg. CPC:\", best_rf_model.score(X_test_cpc, y_test_cpc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechnung des Cross-Validation-Scores für CTR\n",
    "cross_val_scores_ctr = cross_val_score(rf_pipeline, X_train_ctr, y_train_ctr, cv=5)\n",
    "print(\"Cross-Validation Scores für CTR:\", cross_val_scores_ctr)\n",
    "print(\"Average Cross-Validation Score für CTR:\", np.mean(cross_val_scores_ctr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechnung des Cross-Validation-Scores Avg. CPC\n",
    "cross_val_scores = cross_val_score(rf_pipeline, X_train_cpc, y_train_cpc, cv=5)\n",
    "print(\"Cross-Validation Scores für Avg. CPC:\", cross_val_scores)\n",
    "print(\"Average Cross-Validation Score für Avg. CPC:\", np.mean(cross_val_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wichtgste Features für Hypothesentestung ermitteln\n",
    "# Berechnung der Metriken für CTR\n",
    "ctr_pred_train = rf_pipeline.predict(X_train_ctr)\n",
    "ctr_pred_test = rf_pipeline.predict(X_test_ctr)\n",
    "print(\"CTR - Mean Absolute Error (Train):\", mean_absolute_error(y_train_ctr, ctr_pred_train))\n",
    "print(\"CTR - Mean Absolute Error (Test):\", mean_absolute_error(y_test_ctr, ctr_pred_test))\n",
    "print(\"CTR - Mean Squared Error (Train):\", mean_squared_error(y_train_ctr, ctr_pred_train))\n",
    "print(\"CTR - Mean Squared Error (Test):\", mean_squared_error(y_test_ctr, ctr_pred_test))\n",
    "\n",
    "# Berechnung der Metriken für Avg. CPC\n",
    "cpc_pred_train = rf_pipeline.predict(X_train_cpc)\n",
    "cpc_pred_test = rf_pipeline.predict(X_test_cpc)\n",
    "print(\"Avg. CPC - Mean Absolute Error (Train):\", mean_absolute_error(y_train_cpc, cpc_pred_train))\n",
    "print(\"Avg. CPC - Mean Absolute Error (Test):\", mean_absolute_error(y_test_cpc, cpc_pred_test))\n",
    "print(\"Avg. CPC - Mean Squared Error (Train):\", mean_squared_error(y_train_cpc, cpc_pred_train))\n",
    "print(\"Avg. CPC - Mean Squared Error (Test):\", mean_squared_error(y_test_cpc, cpc_pred_test))\n",
    "\n",
    "# Vorbereitung der Daten für die Plotfunktion\n",
    "# Korrekte Extraktion der Feature-Namen aus dem Preprocessor\n",
    "feature_names = [name for transformer in preprocessor.transformers_[:-1] for name in transformer[1].get_feature_names_out()]\n",
    "\n",
    "# Feature Importance aus dem Random Forest Modell\n",
    "feature_importances = rf_pipeline.named_steps['regressor'].feature_importances_\n",
    "\n",
    "# Zuordnung der Feature Importance zu den Namen und Einschränkung auf die wichtigsten 25 \n",
    "importance_dict = dict(zip(feature_names, feature_importances))\n",
    "sorted_importance = sorted(importance_dict.items(), key=lambda item: item[1], reverse=True)[:25]  # Einschränkung hier\n",
    "\n",
    "# Anzeige der sortierten Feature Importance\n",
    "print(\"Top-25 Feature Importance:\")\n",
    "for feature, importance in sorted_importance:\n",
    "    print(f\"{feature}: {importance}\")\n",
    "\n",
    "# Visualisierung der Top-25 Feature Importance\n",
    "def plot_feature_importance(importance, names):\n",
    "    # Create arrays from feature importance and feature names\n",
    "    feature_importance = np.array(importance)\n",
    "    feature_names = np.array(names)\n",
    "\n",
    "    # Create a DataFrame using a Dictionary\n",
    "    data={'feature_names':feature_names,'feature_importance':feature_importance}\n",
    "    fi_df = pd.DataFrame(data)\n",
    "\n",
    "    # Sort the DataFrame in order decreasing feature importance\n",
    "    fi_df.sort_values(by=['feature_importance'], ascending=False, inplace=True)\n",
    "\n",
    "    # Define size of bar plot\n",
    "    plt.figure(figsize=(10,8))\n",
    "    # Plot Seaborn bar chart\n",
    "    sns.barplot(x='feature_importance', y='feature_names', data=fi_df)\n",
    "    # Add chart labels\n",
    "    plt.title('Random Forest - Feature Importance')\n",
    "    plt.xlabel('FEATURE IMPORTANCE')\n",
    "    plt.ylabel('FEATURE NAMES')\n",
    "\n",
    "# Plot der Top-25 Feature Importance\n",
    "plot_feature_importance([imp[1] for imp in sorted_importance], [imp[0] for imp in sorted_importance])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline für Gradient Boosting\n",
    "gb_pipeline_ctr = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"regressor\", GradientBoostingRegressor(n_estimators=100, random_state=42))\n",
    "])\n",
    "gb_pipeline_cpc = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"regressor\", GradientBoostingRegressor(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Training Gradient Boosting für CTR\n",
    "gb_pipeline_ctr.fit(X_train_ctr, y_train_ctr)\n",
    "gb_train_score_ctr = gb_pipeline_ctr.score(X_train_ctr, y_train_ctr)\n",
    "gb_test_score_ctr = gb_pipeline_ctr.score(X_test_ctr, y_test_ctr)\n",
    "print(f\"Gradient Boosting - CTR Train Score: {gb_train_score_ctr}, Test Score: {gb_test_score_ctr}\")\n",
    "\n",
    "# Training Gradient Boosting für Avg. CPC\n",
    "gb_pipeline_cpc.fit(X_train_cpc, y_train_cpc)\n",
    "gb_train_score_cpc = gb_pipeline_cpc.score(X_train_cpc, y_train_cpc)\n",
    "gb_test_score_cpc = gb_pipeline_cpc.score(X_test_cpc, y_test_cpc)\n",
    "print(f\"Gradient Boosting - Avg. CPC Train Score: {gb_train_score_cpc}, Test Score: {gb_test_score_cpc}\")\n",
    "\n",
    "def plot_feature_importance(model, title):\n",
    "    # Auszug der Feature-Namen aus dem Preprocessor\n",
    "    features = [name for transformer in preprocessor.transformers_[:-1] for name in transformer[1].get_feature_names_out()]\n",
    "    importances = model.named_steps['regressor'].feature_importances_\n",
    "    \n",
    "    # Erstellung eines DataFrames zur besseren Handhabung\n",
    "    feature_importances = pd.DataFrame({'Feature': features, 'Importance': importances})\n",
    "    feature_importances = feature_importances.sort_values(by='Importance', ascending=False).head(25)\n",
    "\n",
    "    # Plotting mit Seaborn für eine bessere Visualisierung\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(data=feature_importances, x='Importance', y='Feature', palette='viridis')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Feature Importance für CTR und Avg. CPC visualisieren\n",
    "plot_feature_importance(gb_pipeline_ctr, \"Feature Importance for CTR\")\n",
    "plot_feature_importance(gb_pipeline_cpc, \"Feature Importance for Avg. CPC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_features(model, title):\n",
    "    # Zugriff auf die Feature-Importanzen und die Feature-Namen\n",
    "    importances = model.named_steps['regressor'].feature_importances_\n",
    "    features = [name for transformer in preprocessor.transformers_[:-1] for name in transformer[1].get_feature_names_out()]\n",
    "    \n",
    "    # Kombinieren der Feature-Namen und Importanzen, Sortieren und Ausgeben der Top 5\n",
    "    top_features = sorted(zip(features, importances), key=lambda x: x[1], reverse=True)[:5]\n",
    "    print(f\"Top 5 Features for {title}:\")\n",
    "    for rank, (feature, importance) in enumerate(top_features, 1):\n",
    "        print(f\"{rank}. {feature}: {importance:.4f}\")\n",
    "\n",
    "# Annahme: gb_pipeline_ctr und gb_pipeline_cpc sind bereits trainiert\n",
    "print_top_features(gb_pipeline_ctr, \"CTR\")\n",
    "print_top_features(gb_pipeline_cpc, \"Avg. CPC\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zur Visualisierung der Lernkurve\n",
    "def plot_learning_curve(estimator, title, X, y, axes, ylim=None, cv=None, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    if ylim is not None:\n",
    "        axes.set_ylim(*ylim)\n",
    "    axes.set_title(title)\n",
    "    axes.set_xlabel(\"Training examples\")\n",
    "    axes.set_ylabel(\"Score\")\n",
    "\n",
    "    train_sizes, train_scores, test_scores, fit_times, _ = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes,\n",
    "        return_times=True)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    # Plot learning curve\n",
    "    axes.grid()\n",
    "    axes.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                      train_scores_mean + train_scores_std, color=\"gray\", alpha=0.1)\n",
    "    axes.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                      test_scores_mean + test_scores_std, color=\"g\", alpha=0.1)\n",
    "    axes.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "              label=\"Training score\")\n",
    "    axes.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "              label=\"Cross-validation score\")\n",
    "    axes.legend(loc=\"best\")\n",
    "\n",
    "# Visualisierung der Lernkurven für beide Modelle und beide Zielvariablen\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))  # 2x2 Subplots für 2 Modelle und 2 Zielvariablen\n",
    "\n",
    "# Lernkurven für Random Forest\n",
    "plot_learning_curve(rf_pipeline, \"Learning Curve (Random Forest, CTR)\", \n",
    "                    X_train_ctr, y_train_ctr, axes=axes[0, 0], ylim=(0.7, 1.01), cv=5, n_jobs=4)\n",
    "plot_learning_curve(rf_pipeline, \"Learning Curve (Random Forest, Avg. CPC)\", \n",
    "                    X_train_cpc, y_train_cpc, axes=axes[0, 1], ylim=(0.7, 1.01), cv=5, n_jobs=4)\n",
    "\n",
    "# Lernkurven für Gradient Boosting\n",
    "plot_learning_curve(gb_pipeline_ctr, \"Learning Curve (GBM, CTR)\", \n",
    "                    X_train_ctr, y_train_ctr, axes=axes[1, 0], ylim=(0.7, 1.01), cv=5, n_jobs=4)\n",
    "plot_learning_curve(gb_pipeline_cpc, \"Learning Curve (GBM, Avg. CPC)\", \n",
    "                    X_train_cpc, y_train_cpc, axes=axes[1, 1], ylim=(0.7, 1.01), cv=5, n_jobs=4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
